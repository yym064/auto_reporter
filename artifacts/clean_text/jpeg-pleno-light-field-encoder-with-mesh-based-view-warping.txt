JPEG PLENO LIGHT FIELD ENCODER WITH MESH BASED VIEW WARPING
Yue Li, Reji Mathew, David Taubman
School of Electrical Engineering and Telecommunications
University of New South Wales, Sydney
ABSTRACT
We introduce mesh-based view warping to the JPEG Pleno light ﬁeld
coding framework and replace the standardized sample-based for-
ward warping and splatting of reference texture with mesh-based
backward warping, which allows for a more disciplined interpola-
tion of the reference texture for predicting the target view. Instead
of coding depth maps with JPEG 2000, which is the default option
of the JPEG Pleno framework, we employ a recent extension re-
ferred to as JPEG 2000 Part 17. This extension utilises breakpoints
to describe discontinuity boundary geometry for the purpose of mod-
ifying the predict and update lifting steps in the vicinity of detected
discontinuities. We directly decode breakpoints and corresponding
DWT coefﬁcients onto a mesh and describe a scheme to construct
a single, consolidated mesh for a large group of views, borrowing
information from multiple coded depth maps. Results show that the
cumulative impact of all these modiﬁcations enable improved rate-
distortion performance in comparison with the default operation of
the JPEG Pleno encoder.
Index Terms— Light Fields, Triangular Mesh, DWT
1. INTRODUCTION
The recent JPEG Pleno light ﬁeld coding standard [1] enables a 4D
Prediction Mode (4DPM), where a given target view can be predic-
tively coded using previously decoded reference views. To facilitate
inter-view prediction, a small set of depth maps at select view loca-
tions are also coded, such that depth information along with corre-
sponding camera parameters can be used to calculate the disparity
between views. A prediction is formed by subjecting a set of avail-
able reference views to warping and merging in accordance with the
disparity information determined at the encoder.
In this work we explore the cumulative impact of a number of
modiﬁcations to the default behaviour of the JPEG Pleno light ﬁeld
encoder operating in 4DPM [2] [3]. Instead of defaulting to JPEG
2000 for coding depth maps, we employ a recent extension to the
standard, referred to as JPEG 2000 Part 17 [4], which is speciﬁcally
suited to coding piecewise smooth imagery. The Part 17 extension
employs breakpoints to model discontinuities in the depth ﬂow and
these breakpoints in turn modify the DWT in its vicinity. We refer
to this new transform as breakpoint dependent DWT (BD-DWT).
Another important modiﬁcation relates to representing depth in-
formation with a mesh model to perform mesh-based view warping.
This is in contrast to JPEG Pleno light ﬁeld coding which assumes
decoded depth is sample-based, such that depth samples at each pixel
location are converted to a corresponding disparity vector with the
knowledge of camera parameters. Each disparity vector speciﬁes
the displacement of a pixel from the reference view, where depth is
available, to a target view. The JPEG Pleno encoder uses these de-
rived disparity vectors to forward warp reference texture pixels to a
Fig. 1. (Left) Example of a mesh representation for a depth map of
the Greek dataset. (Right) When a mesh is projected to another view,
tears occur at break-induced discontinuities, leaving holes.
target view. Since warped locations may not necessarily fall at inte-
ger pixel locations at the target view, a splatting process is employed
which involves splatting to the nearest neighbouring pixel location.
The splatting process can cause excessive smoothing or blurring and
does not allow for a disciplined ﬁltering and up-sampling approach.
In this work, breakpoints and wavelet coefﬁcients of a depth map
subject to BD-DWT, are decoded directly onto a mesh. The BD-
DWT is deﬁned on a hierarchical triangular grid allowing for trian-
gular mesh representation which facilitates mesh-based view warp-
ing using triangular cells. The mesh describes a piece-wise contin-
uous representation of depth with breakpoints denoting the bound-
ary geometry along which tears may occur due to occlusion (dou-
ble mappings) or dis-occlusion (holes) when performing view warp-
ing. Importantly, mesh-based warping is invertible, thereby enabling
backward warping which allows for disciplined view interpolation.
A further modiﬁcation which we introduce is the ability to fuse
together depth information from multiple communicated depth maps
to form a single comprehensive base-mesh. For a large group of
views, such as views captured by a high density camera array, mul-
tiple depth maps are often provided. Additional depth maps provide
depth details of the scene that may be hidden or not visible from a
single view point. In our scheme, an initial base-mesh is ﬁrst created
by directly decoding onto a triangular mesh the BD-DWT data of a
single depth map. If further depth maps are available then the base-
mesh is augmented with additional information sourced from these
new depth maps for regions that are not visible from the initial base
view location. A single augmented base-mesh, that can describe the
geometric relationship between views of a large view array, provides
for a consolidated and consistent mechanism for deriving disparity
for view warping.
Rate-distortion (R-D) results show that the cumulative impact of
all three modiﬁcations result in signiﬁcant bit-rate savings. Subjec-
tive results also showcase the advantages of our proposed augmented
base-mesh, allowing for improved prediction at object boundaries
and in dis-occluded regions.
2100
978-1-7281-9835-4/23/$31.00 ©2023 IEEE
ICIP 2023
2023 IEEE International Conference on Image Processing (ICIP) | 978-1-7281-9835-4/23/$31.00 ©2023 IEEE | DOI: 10.1109/ICIP49359.2023.10222269
Authorized licensed use limited to: Korea Electronics Technology Institute. Downloaded on September 05,2025 at 08:06:15 UTC from IEEE Xplore.  Restrictions apply. 

2. RELATION TO PRIOR WORK
Wavelets deﬁned over triangles which are dependent on break-
points (i.e. BD-DWT) and the estimation of these breakpoints have
been described in prior work [5] [6]. Essentially, depth samples
are considered to be located on a triangular grid with lifting steps
performed across the horizontal, vertical and diagonal grid line seg-
ments. Breakpoints which describe boundary geometry are placed
on the arcs of the triangular grid and adapt the predict and update
steps to ensure that the wavelet basis kernels do not cross over de-
tected discontinuity boundaries. The beneﬁts of adopting BD-DWT
to the JPEG Pleno light ﬁeld encoder have been reported in [6]. This
prior work is limited to replacing the default JPEG 2000 coding of
depth maps with the JPEG 2000 Part-17 extension which deﬁnes the
BD-DWT option on a triangular grid; the view warping procedures
remain unchanged. In this paper we build a mesh utilising break-
points and BD-DWT coefﬁcients and employ this mesh to perform
backward warping of texture, replacing the sample based forward
warping and splatting deﬁned by the JPEG Pleno light ﬁeld coding
standard.
Mesh-based reconstruction of wavelet-transformed data has
been considered earlier [7] [8]. More recently, we explored building
a mesh from BD-DWT data [5] with results showing that the piece-
wise afﬁne mesh representation, comprised of triangular cells, can
accurately recreate depth samples at integer pixel locations. With the
number of triangular cells being typically far smaller than the num-
ber of pixels, mesh-based representations also form a more compact
description of the information conveyed by a depth map. In our prior
efforts [9], mesh-based view warping was studied from a purely view
synthesis perspective, omitting the steps of coding. In this work, we
incorporate mesh-based view warping to the JPEG Pleno light ﬁeld
coding framework and report on R-D improvements.
We introduced the concept of a base-mesh in our earlier publi-
cation [10] in which the mesh is anchored or co-located with a com-
municated depth map. The base-mesh investigated in our prior study
[10] is unable to incorporate depth information from additional depth
maps. When performing view warping, regions of dis-occlusions
or holes are ﬁlled in using a backﬁlling strategy which is a self-
inferencing scheme as it relies upon only the base-view depth in-
formation; depth is inferred by extrapolation from the background
side at mesh discontinuities. In this work we are able to augment a
base-mesh with depth information from other depth maps, allowing
for a single consolidated base-mesh to be used for view warping for
the complete view array. Our earlier work [10] was conducted prior
to the standardisation of the JPEG Pleno framework and therefore
the R-D impact of incorporating mesh-based view warping to the
standardized encoder was not previously investigated. Compared to
previous efforts [9] [10], we provide improved dis-occlusion han-
dling with a disciplined mesh augmentation strategy and report on
beneﬁts to JPEG Pleno encoding.
3. CONSTRUCTING A MESH
In this section we provide a brief overview of mesh construction
from BD-DWT data. Further details can be found in [5]. We gen-
erate mesh models through progressive mesh subdivision in accor-
dance with the appearance of non-zero wavelet coefﬁcients or any
novel breakpoints during the BD-DWT synthesis process. We con-
sider subband samples to be located on a coarse to ﬁne triangular grid
with synthesis lifting steps performed across the horizontal, vertical
and diagonal grid line segments at each scale. We start at the coarsest
level of the transform and associate the LL subband data with nodes
Fig. 2. (a) Dt→r derived from base-mesh Mb. (b) Initial base-mesh
M0
b = Mb. (c) Dis-occluded region Ωb→v when M0
b is warped to
view v. (d) Warped mesh elements comprising a new layer Ml
b. (e)
mesh elements of Mv overlapping with dis-occluded region Ωb→v.
{ni} of a mesh, such that these nodes correspond to vertices of tri-
angular mesh elements mj = [ng, nk, nq] whose edges form the
coarsest level horizontal, vertical and diagonal arcs. As coefﬁcients
from other subbands and ﬁner scales are progressively incorporated,
if any non-zero high-pass subband coefﬁcient appears within a trian-
gle mj or on any of its bounding arcs then the mesh element mj is
classiﬁed as being non-afﬁne and subdivided into smaller triangles.
A non-zero update step, from a subband sample located outside a tri-
angle, can also render the mesh element mj non-afﬁne if the update
is applied to a point that is not one of the three nodes (i.e. vertices)
[ng, nk, nq] of the triangle mj. Non-zero updates therefore can also
cause mesh elements to be sub-divided.
Breakpoints derived from the highly scalable JPEG 2000 Part 17
compressed representation provide a piecewise linear description of
discontinuity boundaries. Only a small subset of breakpoints are ex-
plicitly communicated, these are novel breakpoints and termed ver-
tices while the remaining breakpoints are induced along the piece-
wise linear geometry. Mesh elements are partitioned along break-
inducing lines and any subsequent breakpoints induced along this
line at ﬁner levels are not considered novel and therefore do not re-
sult in any subdivision. In contrast, an appearance of a novel break-
point will cause a redeﬁnition of the boundary geometry and will
result in further subdivisions of the triangular mesh elements.
A mesh constructed for a depth map of the Greek dataset is
shown in Fig 1. Regions of smooth depth ﬂow are modelled by large
triangles while regions with more complex variations and non-linear
boundary geometry require sub-division to ﬁner mesh elements. Fig
1 also shows a warped mesh with dis-occluded regions.
4. MESH-BASED WARPING
In the JPEG Pleno coding framework, depth maps are coded only for
a sparse set of views and therefore disparity needs to be inferred at
other remaining views. We create a mesh at a view b for which depth
is communicated and then augment this mesh to form a comprehen-
sive description of disparity for all views. We term view b as a base
view with the corresponding mesh Mb being the base-mesh. We
ﬁrst discuss transporting disparity information from the base view
to reference and target views to allow for backward warping. Spe-
ciﬁc details of dealing with dis-occlusion and mesh augmentation
are presented later.
4.1. Deriving Disparity from a Base-Mesh Model
Using notation Zb to refer to the depth map at view b, the disparity
ﬁeld between b and another view v can be written as
2101
Authorized licensed use limited to: Korea Electronics Technology Institute. Downloaded on September 05,2025 at 08:06:15 UTC from IEEE Xplore.  Restrictions apply. 

Db→v(x) = ¯D(Cb, Cv, x, Zb(x))
where Cb and Cv represent the camera parameters describing the
projective geometry at views b and v respectively, while the trans-
formation ¯D is derived from perspective geometry.
To determine the disparity between any pair of views v and w,
we ﬁrst project each mesh element mb,j of Mb to corresponding
elements mb→v,j by displacing the respective cell nodes according
to nb→v,i = nb,i+Db→v(nb,i). In this notation for mesh elements
and nodes, the ﬁrst subscript identiﬁes the particular base-view at
which the mesh is anchored (i.e., b) or a non-base-view it projects
to (i.e., b →v), while the second subscript corresponds to a unique
triangle-ID j or node ID i. Since mesh elements are triangular, the
mapping of mb,j to mb→v,j is a 2D afﬁne transformation Ab→v,j.
A triangle-ID map T b→v at view v is constructed by assigning
T b→v = j for each location x ∈mb→v,j. When multiple triangles
mb→v,j overlap in view v, the ambiguity due to occlusion (i.e., dou-
ble mapping) is resolved by assigning T b→v(x) to the triangle-ID j
whose depth at location A−1
b→v(x) in view b is smallest. Following
this, the disparity ﬁeld Dv→w is obtained as
Dv→w(x) = Db→w(A−1
b→v,T b→v(x)(x))−
Db→v(A−1
b→v,T b→v(x)(x))
(1)
Fig. 2 (a) illustrates the implementation of equation (1) for de-
termining Dt→r, the disparity ﬁeld from a target view t to a refer-
ence view r using mesh elements anchored at the base view b. From
Dt→r a disparity vector for each pixel in t pointing to a location in
r can be determined, thereby enabling backward warping.
4.2. Mesh augmentation
At the base view, when a breakpoint induced line intersects with
an arc of the triangular grid, new nodes are added to either side of
the discontinuity. Typically, breakpoint induced line segments de-
scribe the boundary between foreground and background content.
New nodes placed on the background side of the discontinuity take
on background depth, extrapolated from values from endpoint arc
nodes on the same side of the discontinuity. Similarly, new nodes
placed on the foreground side take on foreground depth extrapolated
from foreground arc nodes. These nodes on either side of a boundary
are connected together to form inﬁnitesimal mesh elements, which
have no area within the base-view, but expand to identify depth dis-
occluded regions in other views. An example is shown in Fig. 2
(b) and (c). At the base view, in Fig. 2 (b), the breakpoint deﬁned
boundary is shown by the dashed line. Inﬁnitesimal mesh elements
are created at the base view b such that when warped to another view
v, these elements expand to create stretched triangles highlighting
dis-occluded regions Ωb→v.
In our prior work, we performed a backﬁlling strategy where at
a non-base view v, expanded inﬁnitesimal mesh elements are ﬁlled
with background depth. This is performed by replacing foreground
node values of the expanded triangles at v with extrapolated back-
ground depth values. These background ﬁlled mesh elements are
then warped from view v to the base view b to form a new layer of
mesh elements. The initial base-mesh model Mb is augmented with
multiple layers, with each layer associated with a unique layer-ID l.
ˆ
Mb =
[
l=0,1,2...L
Ml
b
(2)
The augmented base-mesh
ˆ
Mb is a union of mesh elements from
all layers, as shown by Equation (2). Mesh elements associated with
layer-ID l = 0 belong to the initial base-mesh derived from Zb; that
is M0
b = Mb. Subsequent layer-IDs l correspond to new elements
derived from backﬁlling when warping to a particular view in the
view array. When the augmented model
ˆ
Mb is warped to a given
view, many mesh elements may map to any given location within
that view. We resolve such multiple mappings by assigning absolute
priority to mesh elements with lower layer-ID l.
Backﬁlling is a self-inferencing scheme, relying on only the
base-view depth information Zb. In this work we pursue a layered
representation which is able to incorporate depth information from
other communicated depth maps. The process is illustrated in Fig.
2 (d) and (e). If in addition to the base-view b, a depth map is also
communicated at another view v then a new independent mesh Mv
is created at v in accordance with Zv. The original base-mesh Mb
is warped from b to v with expanded inﬁnitesimal mesh elements
identifying dis-occluded regions Ωb→v in v. Mesh elements mv,j
of the new mesh Mv which intersect with Ωb→v are identiﬁed and
denoted as {mv,j}l, such that
{mv,j}l = {mv,j ∩Ωb→v ̸= ∅}
This set of elements {mv,j}l are then warped to the base view to
create a new layer for the augmented representation with layer-ID
l > 0 as shown below.
Ml
b = {mv→b,j}l
As before, the augmented mesh is created in accordance with equa-
tion (2), with the original mesh at b forming the set for layer-ID
l = 0 (M0
b = Mb). If only the base-view depth map is available,
or if the communicated depth information is insufﬁcient to ﬁll dis-
occluded regions at certain views, then our scheme defaults back to
employing backﬁlling. In Fig. 2 (e), the grey shaded grid represents
mesh elements {mv,j}l anchored at the non-base view v that inter-
sects with Ωb→v. These triangular cells are warped to the base-view
b as shown in Fig. 2 (d) to create a new layer in addition to the ini-
tial mesh in Fig. 2 (b). In this layered representation, dis-occluded
regions at layer l are potentially ﬁlled with triangles emerging at
subsequent layers with layer-ID l + 1 or greater.
5. EXPERIMENTAL RESULTS
We modify the JPEG Pleno Light Field Veriﬁcation Model 2.1 to
incorporate (i) BD-DWT coding of depth maps as deﬁned by JPEG
2000 Part 17, (ii) mesh-based backward warping and view prediction
and (iii) base-mesh augmentation using backﬁlling and when possi-
ble augmentation using multiple depth maps. R-D results comparing
the cumulative impact of these modiﬁcations to the default operation
of the state-of-the-art JPEG Pleno light ﬁeld encoder is presented in
Fig. 3. We follow a hierarchical coding structure, as typically em-
ployed by the JPEG Pleno coding standard when operating in 4DPM.
At the coarsest level a small number of texture views and depth maps
are intra-coded. Views at subsequent levels are coded with reference
to previously decoded views from coarser levels.
We report on target bitrates (bpp) inclusive of the operating
ranges speciﬁed by the Common Test Conditions (CTC) deﬁned by
JPEG Pleno [11]. We present results for synthetic HCI light ﬁeld
datasets Dishes, Greek and Pens [12], along with the Set2 dataset
[13] which is captured by a high density camera array. The HCI
scenes are comprised of a 9 × 9 array of views with a resolution
of 512 × 512. These datasets are coded, at the coarsest level, with
one ground-truth depth map and one intra-coded texture view at the
center viewpoint [4, 4]. Set2 is composed of views of resolution
2102
Authorized licensed use limited to: Korea Electronics Technology Institute. Downloaded on September 05,2025 at 08:06:15 UTC from IEEE Xplore.  Restrictions apply. 

Fig. 3. Rate-distortion results. Average PSNRY for a range of rates expressed in bits per pixel (bpp)
1920 × 1080 with the original size of the array being 99 × 21
(cols × rows). We subsample this large array by 6 in the horizontal
direction and 2 in the vertical direction and consider only the central
9 × 7 sub-sampled array for coding. We note that there are large
disparities between views in this dataset and therefore the quality of
view warping and prediction is key to performance. For Set 2, at the
coarsest level, we code 4 depth maps along with 5 intra-coded tex-
ture views - one at each corner along with the centre view location.
All datasets are coded with 5 levels of hierarchy.
In Fig 3, (red) curves labelled brkMesh-bddwt correspond to our
proposed augmented mesh model for backward warping of texture
along with BD-DWT coding of depth maps. This case represents
incorporating all our proposed modiﬁcations to the standard JPEG
Pleno encoder. Curves (gray-dashed) labelled imgSamp-affdwt re-
fer to the default operation of using sample-based depth to perform
forward warping with splatting and employing the conventional 5/3
DWT for coding depth maps. We note that the 5/3 DWT is also
well suited to represent smooth afﬁne ﬂows and hence we have de-
ployed the name affdwt in our labelling convention. We highlight
that imgSamp-affdwt cases correspond to the default operation of
the JPEG Pleno encoder. R-D performance improvements achieved
by our proposed modiﬁcations are clearly evident for all 4 datasets
when comparing curves brkMesh-bddwt and imgSamp-affdwt.
Curves (black) labelled imgSamp-bddwt refer to cases where the
default sample-based forward warping and splatting is employed but
the coding of depth maps is changed to use the BD-DWT. The mod-
iﬁcation to the JPEG Pleno encoder is therefore limited to switch-
ing from the 5/3 DWT to the BD-DWT. Performance gains in rela-
tion to default operation (imgSamp-affdwt) is solely due to improved
compression of depth maps and this has been previously reported.
By comparing curves brkMesh-bddwt with imgSamp-bddwt, we can
gauge the performance enhancements achieved due to our proposed
mesh representation and backward warping of texture.
For Set 2, more than 1 depth is communicated and therefore our
new mesh augmentation strategy that can incorporate depth infor-
mation from multiple depth maps is employed. The (orange) curve
labelled brkMesh-bddwt-bfonly shows performance for Set 2 when
mesh augmentation is limited to backﬁlling while brkMesh-bddwt
includes our new mesh augmentation strategy that supplements the
base-mesh with information from all 4 depth maps. While the back-
ﬁlling strategy works well, our new approach provides further gain.
To provide further insight to our proposed modiﬁcations, we
consider coding only the coarsest level which includes intra-coded
texture views and depth maps and then synthesising the remaining
views at the ﬁner levels. This coding scenario is chosen to high-
light view synthesis capability using a common set of high quality
decoded reference texture views and depth maps. In Table 1 SSIMY
Table 1. Average SSIMY of synthesized light ﬁeld views. Shown in
brackets are differences to the benchmark imgSamp-affdwt
Dataset
Bitrate
imgSamp-affdwt
brkMesh-bddwt
(bpp)
Dishes
0.069
0.9528
0.9695(+0.0167)
Greek
0.052
0.9419
0.9593(+0.0174)
Pens
0.040
0.9314
0.9609(+0.0295)
Set2
0.166
0.9681
0.9771(+0.0090)
Fig. 4. Crop of target view [4, 8] synthesized from decoded coarsest
level of Greek scene: (left) imgSamp-affdwt, (right) brkMesh-bddwt
results are shown, computed by averaging the SSIMY values of indi-
vidual views, as recommended by the JPEG Pleno CTC [11]. Higher
SSIMY values for brkMesh-bddwt suggest that scene object bound-
ary structure is better synthesized by our proposed modiﬁcations. A
crop of a synthesized view of the Greek dataset is shown in Fig. 4;
cleaner and sharper object boundaries are recreated by our proposed
set of modiﬁcations.
6. CONCLUSION
We modify the JPEG Pleno light ﬁeld encoder to incorporate break-
point dependent DWT (BD-DWT) coding of depth maps as deﬁned
by the recent JPEG 2000 Part 17 extension. The components of
the BD-DWT are decoded directly onto a triangular mesh allowing
for mesh-based backward view warping and prediction. We further
introduce a strategy to augment the mesh representation to include
depth information from multiple communicated depth maps. Results
show that the modiﬁcations which we apply to the JPEG Pleno light
ﬁeld encoder provide signiﬁcant gains in rate-distortion performance
compared to the default operation of the standardized encoder.
2103
Authorized licensed use limited to: Korea Electronics Technology Institute. Downloaded on September 05,2025 at 08:06:15 UTC from IEEE Xplore.  Restrictions apply. 

7. REFERENCES
[1] “Information technology - jpeg pleno plenoptic image coding
system - part 2: Light ﬁeld coding,”
2019,
ISO/IEC JTC
1/SC29/WG1N84065.
[2] P. Schelkens, P. Astola, E. A. B. da Silva, C. Pagliari, C. Perra,
I. Tabus, and O. Watanabe,
“JPEG Pleno light ﬁeld coding
technologies,”
in Applications of Digital Image Processing
XLII. International Society for Optics and Photonics, 2019, vol.
11137, pp. 391 – 401, SPIE.
[3] C. Perra, P. Astola, E. A. B. da Silva, H. Khanmohammad,
C. Pagliari, P. Schelkens, and I. Tabus, “Performance analysis
of jpeg pleno light ﬁeld coding,” in 2019 SPIE Optics and Pho-
tonics, Applications of Digital Image Processing XLII, 2019.
[4] “Information technology — jpeg 2000 image coding system —
part 17: Extensions for coding of discontinuous media,” 2023,
ISO/IEC JTC 1/SC29/FDIS 15444-17.
[5] Y. Li, R. Mathew, and D. Taubman, “Scalable mesh represen-
tation for depth from breakpoint-adaptive wavelet coding,” in
2020 IEEE International Workshop on Multimedia Signal Pro-
cessing (MMSP), 2020.
[6] R. Mathew and D. Taubman, “Jpeg pleno light ﬁeld encoder
with breakpoint dependent afﬁne wavelet transform for dispar-
ity maps,” in 2022 IEEE International Conference on Image
Processing (ICIP), 2022.
[7] M. Bertram, M. A. Duchaineau, B. Hamann, and K. I. Joy,
“Generalized b-spline subdivision-surface wavelets for geome-
try compression,” IEEE transactions on visualization and com-
puter graphics, vol. 10, no. 3, pp. 326—-338, 2004.
[8] L. Linsen, B. Hamann, K. I. Joy, V. Pascucci, and M. A.
Duchaineau, ““wavelet-based multiresolution with pn 2 sub-
division,” Computing, vol. 72, no. 1-2, pp. 129—-142, 2004.
[9] Yue Li, Reji Mathew, Dominic Ruefenacht, Aous Naman, and
David Taubman, “Consistent disparity synthesis for inter-view
prediction in lightﬁeld compression,” in 2019 Picture Coding
Symposium (PCS), 2019, pp. 1–5.
[10] Dominic R¨ufenacht, Aous Thabit Naman, Reji Mathew, and
David Taubman,
“Base-anchored model for highly scalable
and accessible compression of multiview imagery,”
IEEE
Transactions on Image Processing, 2019.
[11] “Jpeg pleno light ﬁeld coding common test conditions,” 2018,
ISO/IEC JTC1/SC29/WG1, WG1N80027.
[12] K. Honauer, O. Johannsen, D. Kondermann, and B. Gold-
luecke, “A dataset and evaluation methodology for depth esti-
mation on 4d light ﬁelds,” in 2016 Asian Conference on Com-
puter Vision. 2016, pp. 19–34, Springer.
[13] Fraunhofer IIS,
“Static planar light ﬁeld test dataset,”
in
https://www.iis.fraunhofer.de/en/ff/amm/dl/lightﬁelddataset.html
[online].
2104
Authorized licensed use limited to: Korea Electronics Technology Institute. Downloaded on September 05,2025 at 08:06:15 UTC from IEEE Xplore.  Restrictions apply. 
